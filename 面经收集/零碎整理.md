## 多级缓存的数据一致性

常见的多级缓存模式为 **本地缓存->分布式缓存->数据库**，对于数据库和分布式缓存的同步策略只需要写后删除即可，当然，如果需要应对删除失败的情况，需要重试，就得用消息队列了，而本地缓存也需要通过消息队列通知其他节点将这个缓存删除，当然，我们也可以通过 CDC 监听 binlog 来实现 Redis 缓存删除（原因是实现可靠的删除）。

当然 mq 是可能丢消息的，我们也不可能为了消息不丢就做个 outbox，我们还可以给本地缓存指定一个相对比较小的 ttl，防止 mq 消息丢失导致脏数据一直滞留。

即便是这样，也有可能会有不一致的情况，比如一个请求发现本地和分布式缓存都没有数据，然后进入到数据库读取数据，打算回填缓存，此时，另一个请求将新数据写入数据库，然后出发写后删除策略，发给消息队列，此时执行了删除策略，随后，回填了缓存，此时就会产生不一致，所以有一个策略叫做延迟双删策略，通过这个策略可以更大程度保证一致性。

除此之外，还可以使用版本号的策略，通过 mq 广播版本号，让每个节点都同时删除过期的缓存，这样，即便不需要延迟双删也能够防止读到旧数据，当然，对于本地里面没有的 cache，可能意味着我们需要保存多余的数据，我们可以为版本号设置一个合理的过期时间，来保证版本更新之前不会有未执行完的请求，这也是延迟双删所干的事情，但是延迟双删依赖于时间窗口，依旧具有不确定性，但是相对来说已经够可靠了，大多数场景下，短 ttl + 延迟双删都够用了，而版本号能够实现高准确的缓存一致性，但是需要对现有表进行修改，在后期引入会对表有一定侵入性。


## 有栈协程和无栈协程

参考文献：[有栈协程和无栈协程](https://zhuanlan.zhihu.com/p/347445164)

有栈协程就是在用户态去模拟内核干的事情，即在自己的协程栈中去保存自己的寄存器上下文，然后就可以切换到另一个 goroutine 执行，等到需要再次运行这个协程的时候，就从这个协程的栈中恢复。

而无栈协程，意味着每一个协程没有单独的栈去保存他们的上下文，我的理解是编译器将一个异步函数分割成了 N 个阶段，就像函数调用一样去调度这些 task，一个有栈协程，可能会是这样写的，我们需要通过 poll 去执行协程切换：
```c
void* test(void* para){
	co_enable_hook_sys();
	int i = 0;
	poll(0, 0, 0. 1000); // 协程切换执行权，1000ms后返回
	i++;
	poll(0, 0, 0. 1000); // 协程切换执行权，1000ms后返回
	i--;
	return 0;
}

int main(){
	stCoRoutine_t* routine;
	co_create(&routine, NULL, test, 0);// 创建一个协程
	co_resume(routine); 
	co_eventloop( co_get_epoll_ct(),0,0 );
	return 0;
}
```
而对于无栈协程，他可能会是这样去执行的：
```c
struct test_coroutine {
    int i;
    int __state = 0;
    void MoveNext() {
        switch(__state) {
        case 0:
            return frist();
        case 1:
            return second();
        case 2:
        	return third();
        }
    }
    void frist() {
        i = 0;
        __state = 1;
    }
    void second() {
        i++;
        _state = 2;
    }
    void third() {
    	i--;
    }
};
```
看起来其实就是一个函数执行（个人感觉无栈协程好抽象，看了几个 js 的 await 感觉可读性有点差），将原本需要进行协程切换的地方作为分割点，分成 N 个阶段去执行，每次执行完之后，会改变 state 字段，下一次调用就会执行下一个阶段，所以协程切换实际上并不需要一个专门的栈去临时保存上下文，所以上下文切换的开销非常小，同时他直接使用系统栈，遵循了局部性原则，性能会更高，这也是无栈协程的优势之一，但是这也意味着无栈协程难以做到真正的抢占式调度，所以我们写代码的时候还得非常注意这一点。

而 Go 中则是由 runtime 在一些 safe point 注入的代码中实现抢占，调度的逻辑，可以说这一点还是比无栈协程好的。而相对来说，无栈协程的上下文切换的开销很小。


## CAP 和 BASE

讲 [CAP](https://cloud.tencent.com/developer/article/1860632) 的文章。

## 新版的雪花算法

[新版雪花算法](https://seata.apache.org/zh-cn/blog/seata-analysis-UUID-generator/)，这主要是 seata 内部用的，它主要做出了两个改变，一个是不再严格依赖系统时间，二是交换时间戳和节点 id 的位置。

那么为什么说它不再严格依赖系统时间呢？在原版的雪花算法中，我们知道如果当前**时间戳的序列号用完**了，我们必须要等待下一个时间戳的到来才可以生成 id，那么新版的雪花算法则通过借用下一个时间戳来生成 id，以此来避免了等待的问题，同时为了方便这个进位，新版算法将节点 id 和时间戳的位置互换了，这就有了下一个问题。当然这种超前消费有人觉得可能会导致严重后果，但是其实影响并不大，只有并发量极大的时候才有可能导致重启时 id 重复，然而这种极大的并发量就算是 redis 也扛不住，更别提 mysql 了。还有一个问题，就是**时间回拨**，在运行期间，新版雪花算法只会在刚开始会获取一遍系统时钟，之后不再保持同步，只有重启时人为回拨或修改操作系统时区才可能产生回拨导致的问题。

第二个问题是为了方便时间戳的序列号用完方便进位所以才交换的，这就导致了他在单节点内部虽然是单调递增的，但是分布式多节点就不是了，他将时间戳和 workerId 互换了位置，这样的场景下，比如在写入 mysql 的时候，会因为插入的数据不是单调递增的，而产生页分裂问题，但是新版的雪花算法虽然不是单调递增进行分配，但是它也恰恰减少了页分裂。为什么呢？

之后，随着写入的数据量增大，我们会发现每次新插入的节点会在不同的链表块的页的尾部进行追加，就没有页分裂的情况了，此时就能达到一个稳定的状态，每次新插入的数据就只会在对应节点的子序列的尾部进行追加数据，在达到稳定之后，也能产生全局顺序递增的效果。
