# SQL碎片知识

本篇笔记目的在于记录常见的 MySQL 相关的问题。

## 1. 慢查询怎么处理？

查看慢查询日志，通过 explain 分析慢查询的 SQL 语句，查看是否走的全表扫描，或者没有利用索引，随后添加合适的索引，同时查询需要遵循最左匹配原则。

同时应该检查隐式转换的问题，比如

```sql
# phone 为 VARCHAR 类型
SELECT * FROM user WHERE phone = 2;
```

此时会将 phone 转换为 int，而不是将 2 转换为 varchar！由于 varchar 比较方法和数值不一样，如果此时走索引的代价无异于新建一个索引，所以此时会触发索引失效，而走全表扫描。与此同时，函数也是一个原理，当我们对 phone 套一层函数时，由于函数输出并不确定，使得索引也失效了，这一点和类型转换原理上差不多。当然，mysql 也存在优化器会对有些语句进行改写，是的还是可以走索引的。

另外也可以进行简化查询，只查询必要的字段，或者使用覆盖索引，或者减少通配符的使用；使用 JOIN 替代嵌套的 SELECT 减少子查询。

如果发现这个查询确实走了索引，但是依旧很慢，可能需要分析锁。

## 2. 为啥 MySQL 选择了 B+ 树而不是平衡二叉树或者 B 树？

普通的二叉搜索树在顺序插入索引的时候会导致树退化成链表，这种情况下就相当于全表扫描。

而平衡二叉树或者红黑树性能差异并不大，但是由于每一个节点仅能存储一个索引，所以存储效率很低，在存储成千上万的索引时，树的高度很大，而每一次查找都是一次磁盘 I/O ，所以我们期望高度会比较低的数据结构。

B 树和 B+ 树的高度都比较低，为什么没有选择 B 树？B 树的数据是分布于整个树中的，除了叶子节点，还分布于内节点上面，不利于我们走范围查询，并且查询的速度波动很大，除此之外，这样的性质使得 B+ 树可以存放更多的索引，查询效率更高，磁盘的 I/O 也会更少。

而我们的 B+ 树，它的数据都存储在叶子节点上，而内节点都用于放索引，同时，我们的叶子节点相邻之间构成了双向链表，这种设计非常利于范围查询。 B+ 树存在大量的冗余节点（内节点）这就是的插入和删除的效率很高，树形结构的变化很小，而 B 树由于没有冗余节点，所以删除就会非常复杂，树形变化较大。

## 3. 聚簇索引和二级索引？

一张表只有一个聚簇索引，聚簇索引的叶子节点上存储着完整的数据，而二级索引存储的是对应的“主键”字段，当我们通过二级索引去查询数据时，就会先通过二级索引拿到主键信息，然后根据主键回到聚簇索引之中去查询具体的数据，也就是说，要执行两次 B+ 树查询，这种情况就是**回表**。

当然，如果我们查询的数据已经包含在了索引之中，就无需回表查询，这种情况就是**索引覆盖**。

## 4. MyISAM 和 InnoDB 的区别

MyISAM 的索引是非聚集索引，数据和索引分离，每一个数据查询都需要经过回表的步骤，而 InnoDB 有聚集索引，通过主键检索的效率很高。

MyISAM 最细粒度的锁是表锁，而 InnoDB 支持行锁，并发更强。

MyISAM 不支持外键和事务。

## 5. 为什么会有最左前缀原则？

我们建立联合索引的时候，往往会有一个顺序，比如联合索引（name，age，email），我们的索引只有一个，如何根据这个联合索引去建立这唯一的索引呢？我们是先按照 name 排序，再按照 age，最后按照 email 来排序的顺序，所以如果我们直接使用 `SELECT * FROM users WHERE age = 12` 的时候，我们直接去寻找年龄为 12 的用户，但是此时我们并没有根据 name 去查找，而是直接根据 age，那么问题来了，如果我们真的去走这个联合索引，我们找到了一个 `age = 12` 的用户，但是这个是唯一的 `age = 12` 的用户吗？很有可能不是，我们根本不可能根据 `age = 12` 这个条件从这个联合索引中去找到所有的 `age = 12` 的用户，所以此时，只能走全表扫描，这就是最左前缀的由来。

## 6. B+ 树里面的数据是怎么存储的？怎么查找的？

B+ 树中的每个节点是以页为单位的，不论是叶子节点的数据还是内节点的索引，都是以页为单位的，都是一个数据页， MySQL 中的页的单位是 16 kb ，这也是和很多操作系统不一样的地方，这也是为什么 B+ 树可以存储比 B 树更多的索引，变得更矮胖，查询效率更高，磁盘 I/O 次数更少。

我们的索引在页中也是链表的形式串联的，但是会进行分组，每个组仅有几个元素，每个组都有一个目录，我们的目录会记录每个组的最后一个节点的数据和指针，而页目录可以帮助我们做二分查找，我们可以进行一次磁盘 I/O ，将这一页加载到内存中，然后通过二分查找，从而实现快速的查询，然后找到下一层节点的索引，进行下一步加载。

最后，我们进入到了叶子节点，也就是挂着数据的节点，每个行数据都是和对应的索引绑定在一起，也就是说，结构是一样的，还是通过链表绑在一起，我们可以先将叶子节点加载到磁盘中，然后通过页目录进行二分查找，最终找到对应的行。而在分组内，肯定只能实现遍历查找了。

这里可以引出另一个问题，**为什么是 B+ 树？**我们根据刚刚提到的，我们可以直接使用利用页目录进行二分查找，为什么不把所有的数据组织成一个链表呢？这是因为我们希望加载很方便，一次只需要加载一个页，便于维护，而我们的每个页只需要一个页目录就可以了，然而，一个页的大小，不可能承受无限的页目录，所以我们需要多个页，这个时候又会出现一个问题，我们应该先在哪一个页进行二分？先将哪个页加载到磁盘？所以我们只能向上扩展一层作为上一层索引，一次来判断去加载哪一个页，而此时的结构，就和我们的 B+ 树很像了，随着数据的增长，我们的 B+ 树的高度也会长高，而为了数据检索更快，变高是不可避免的，这不就是我们刚刚说的模型吗？这就是 MySQL 使用 B+ 树的由来。

并且，如果对于二叉树，我们每次加载一页到内存，这个页对应的节点可能并不连续，可能导致多次磁盘IO，效率也很低下。相比之下 B+ 树稳定的最多三次磁盘 I/O ，性能和稳定性都更好。



## 7. 索引下推优化是什么?

对于我们的联合索引 `index(a, b)` ，在执行 `SELECT * FROM USERS WHERE a > 1 AND b < 2` 的时候，如果没有索引下推，则会先找到符合 `a > 1` 的数据，然后回表回到数据页去判断 'b < 2' 的条件，这就导致了有一些数据没能利用到第二个 b 索引进行过滤，所以在新的 MySQL 版本中，引入了索引下推，使得我们的 sql 语句在第一步索引检索期间，回表之前就可以通过 a 和 b 两个字段直接进行过滤，以此来减少回表操作。当然，前提是索引中包含对应的字段。	

## 8. 什么时候适合/不适合用索引？

索引也有一些缺点：

1. 占用物理空间。
2. 创建和维护索引耗时，尤其是数据量很大的时候。
3. 增删改的效率降低，因为 B+ 树需要进行动态维护。

在这些场景下，并不适合建立索引：

1. 在 `WHERE`, `ORDER BY`，`GROUP BY` 中很少用到的字段，数据重复多的，比如性别这种字段，但是像 uid 这种唯一字段，或者商品编码比较适合建立索引。
2. 表数据少的时候。
3. 经常更新的字段，比如账户余额。（因为会频繁的更改，影响数据库性能）

## 9. 索引有什么优化的办法？

**前缀索引优化**

- 前缀索引优化其实就是使用字符串的前几个字符建立索引，使用前缀索引可以避免使用整个很长的字符串进行索引，从而节省我们的索引的空间，使得可以在相同的磁盘空间中建立更多的索引，同时因为字符变短，检索也更快。但是需要你的前缀有一定的区分度。前缀索引可以用来优化模糊匹配和加速 WHERE 查询。

**覆盖索引优化**

- 代表我们通过 SELECT 选择的字段直接在二级索引的叶子节点上就可以找到而无需回表操作。

**主键索引自增**：

- 这样在插入数据时可以减少 B+ 树的自适应旋转，由于我们都是插入自增的数据，所以主键索引都是以追加的形式放在 B+ 树中，而减少了结构的变化导致的性能问题。而如果插入的是非自增主键，每次的索引值都是随机的，需要插入现有的数据到索引的中间，我们就不得不去复制移动其他的数据到另一个页，这种情况叫做**页分裂**，这也有可能造成大量的内存碎片，导致索引结构不紧凑，影响查询效率。

**索引设置为 NOT NULL**：

- 虽然感觉没人会设置为 NULL，但是这样做有两个原因，第一是因为索引列存在 NULL 值导致优化器做选择的时候会更复杂，更难以优化，比如索引的统计和值都很复杂，因为是 NULL ，没人知道你是在干嘛。二是 NULL 没有意义，并且会占用物理空间。

**防止索引失效**：

- 当我们使用左或者左右模糊匹配会导致索引失效，因为我们只能根据前缀去匹配字符串。
- 使用函数也会导致索引失效，因为我们索引保存的是原始值，而不是经过函数计算的值。
- 对于索引进行表达式计算也会导致索引失效，这里和使用函数差不多，比如：`SELECT * FROM users WHERE id + 1 = 10` 此时如果我们将 +1 移动到右侧，索引就可以生效了。

- 值得一提的例子，MySQL 会自动把字符串转换为数字，如果我们的索引项是字符串的时候，我们传入一个数字，则会导致索引失效，因为此时是对索引项进行类型转换，也就是使用了函数，但是如果索引项是数字，我们传入字符串，索引并不会失效，因为此时是对我们传入的字符串进行类型转换，并没有对索引做操作。
- 另外对于 OR 语句，如果两个字段其中一个不是索引字段，就会走全表扫描，这一点很容易理解。
- 需要遵循最左匹配原则。

## 10. count(1)，count(*)，count(其他字段)，到底谁性能更好？

答案是：**count(*) = count(1) > count(主键) > count(其他)** 

count() 的作用是统计不为 NULL 的字段个数，我们的 count(*) 并不是去读取所有字段，我们的 MySQL 会将 count(\*) 转换成 count(0) 来处理， 他和 count(1) 一样，都是通过循环遍历聚簇索引而不会读取任何字段的值，相比于 count(主键) 少了一个读取字段值的步骤，而其他的都需要读取字段值，确认是否为 NULL ，才能对 count + 1，但是如果表中存在二级索引，我们就会去循环遍历二级索引，因为二级索引占的空间更小，I/O 成本也就更小。

## 11. 事务的性质？

1. 原子性：一个事务中的所有操作，要么全部完成，要么全部失败。（undo log 实现）
2. 一致性：事务在提交前和提交后，需要满足一致性约束。（其他三者实现）
3. 隔离性：事务应该可以防止多个事务并发执行时导致的数据不一致，不会相互干扰。（MVCC 实现）
4. 持久性：事务执行完成之后，数据持久保存。（redo log 实现）

## 12. 事务会引发哪些问题？

由于 MySQL 可以同时处理多个事务，此时就会有三个问题：

1. 脏读：读取别人还没有提交的事务中更改的数据。
2. 不可重复读：在同一事务中前后读取的**数据**不一致，也就是说，在第一次读取之后，另一条事务修改这个数据并提交之后，再次读取，发现两次数据不一致，这就是不可重复读。
3. 幻读：同一事务中读取到的结果集的**记录数量**不一致。

对于这三种问题， MySQL 有四种隔离级别：

1. 读未提交（read uncommitted）：一个事务还未提交的时候，它的变更能被其他事务看到，相当于直接读取最新的数据。此时，三个问题都有。
2. 读已提交（read committed）：一个事务只能看见另一个事务已经提交的数据，相当于创建了一个 read view ，每次事务提交都会创建一个 read view 快照。这里仅仅解决了脏读的问题。
3. 可重复读（repeatable read）：在一个事务中读取的数据总是和之前读取的数据一致，相当于仅在事务启动时候的数据做了 read view 快照。可重复读解决了脏读和不可重复读的问题，但是存在幻读，这也是 MySQL 默认的事务隔离级别。
4. 串行化（serializable）：加锁，同一时刻只能有一个事务。解决了所有的问题，缺点是没有并行度，性能差得不行。

其中，可重复读可以一定程度上避免幻读，但是无法完全避免：

- **快照读（普通 select 语句）**：通过 MVCC 的方式解决了幻读，事务执行过程中看到的数据总是一致的。
- **当前读**：通过 next-key lock 阻止其他执行向这个范围插入数据，我们可以通过 `select ... for update` 加锁，当前读只能通过加锁来解决，因为它本身就是并发写引起的不可避免的问题。

## 13. MVCC 里面的 read view 是什么，怎么工作的？

我们在讲事务的时候提过 read view ，通过 read view 我们可以实现不同的隔离级别。

我们的 read view 有四个字段：

> - m_ids：是一个事务的 id 列表，其中包含的事务已经启动，但是还没有提交。
> - min_trx_id：创建 read view 的时候，m_ids 中的 id 最小值，最早创建的事务。
> - max_trx_id：跟上面相反，表示 m_ids 中的最大值的下一个id，也就是最大值 +1。
> - creator_trx_id：创建这个 read view 的事务的 id。
>

除此之外，我们的 MySQL 中还有两个隐藏列：

>
> - trx_id：当一个事务对聚集索引记录进行改动的时候，就会把这个事务的 id 记录在这个隐藏列里面。
> - roll_pointer：每次对索引记录进行改动时，都会把旧的版本记录写到 undo log 里面，这个指针就指向旧的版本记录，所有的旧版本都以链表的形式串联起来，这样利于我们后续选择合适的版本来进行读取。

当一个事务去访问一条记录的时候：

> - 如果这个记录的 `trx_id` 小于 `read view` 的 `min_trx_id` ，则说明这个版本的记录是在这个事务创建之前已经提交的事务生成的，可见。
> - 如果大于 `max_trx_id` ，则是这个 `read view` 创建之后才启动的事务改变的，不可见，所以进而去沿着 undo log 链路去访问这个记录的下一个版本。
> - 如果在这之间，则还需要判断这个 `trx_id` 是否在 `m_ids` 列表中，如果在，则说明这个事务还没有提交，不可见，访问旧版本。如果不在，则说明已经提交，可见。

这就叫做多版本并发控制**MVCC**，尽管串行化才能完美解决所有的事务并行引发的问题，但是性能非常差，而可重复读尽管无法完全避免幻读，但是仍可以很大程度避免幻读现象，所以最好还是采取可重复读的隔离级别。

除了与可重复读有关，我们的 undo log 还可以实现事务回滚。

## 14. buffer pool 是啥，有啥用？

MySQL 启动时，会申请一段连续的内存空间作为 buffer pool，并且按照 16 kb 一页分成很多个页，此时是空闲页。

当我们从磁盘中读取数据时，会先将数据从磁盘加载到 buffer pool 里面，然后才会进行读取；而对于写操作，也是会将数据从磁盘加载到 buffer pool 中，并且更新一条记录之后，并不会将他直接写入磁盘中，而是在 buffer pool 中将他缓存起来，相当于在磁盘前面加了一层 cache ，使得读取速度更快。

MySQL 中的数据是以页为单位存储的，buffer pool 也一样，所有的数据都是按照页来缓存的。当我们写入数据时，如果 buffer pool 中存在这个页，就直接写入 buffer pool ，标记为脏页，后台线程选择合适的时机写入磁盘。

它不仅会缓存数据页和索引页，还会缓存 undo 页等等页。

**深入理解**，buffer pool 是一段连续的内存空间，运行了一段时间之后，肯定既有空闲的，也有缓存数据的页，此时我们会维护一段空闲内存的 Free 链表，就和 xv6 做的一样。

同时，对于脏页，我们也会维护一个 Flush 链表，来快速的知道哪些页是脏页，这样有利于后台线程快速地将脏页写入到磁盘。

如果想要提高缓存击中率，我们可能会想到 LRU 策略，但是 MySQL 并没有采取原始的 LRU 缓存策略，因为单纯的 LRU 会引起预读失效和 buffer pool 污染：

> 1. **问题**：MySQL 在加载数据页的时候会将相邻的数据页加载进 buffer pool ，这就叫做预读，而对于预读，我们使用 LRU 算法的话，可能会导致频繁读取的数据反而被替换，而很少读取的页很久才被替换，但是我们并不能直接去掉预读的机制，我们可以选择将预读的页留在缓冲区的时间尽可能短。
>
>    **解决**：MySQL 将 LRU 划分为两个区域，old 区域和 young 区域，young 区域处于链表的前面一部分，而 old 在后面，我们预读的页就会加载到 old 的头部，而真正被访问的页就会插入 young 的头部，以此来实现预读停留时间少，减少预读失效的情况。
>
> 2. **问题**：buffer pool 污染也是一个问题，当我们 MySQL 扫描了大量的数据，而 buffer pool 有限，导致了大量的热数据被替换出去，此时再次访问这些热数据，就会产生大量的磁盘 I/O，比如全表扫描，很多缓冲页只会访问一次，但是却直接进入了 young 区域，而替换了热点数据，所以我们可以提高进入 young 区域的门槛。
>
>    **解决**：为进入到 young 区域的页提高门槛，将第一次访问 old 中的某一个节点的时候，和后续访问节点的时间做对比，如果超出一个阈值，就会被放入 young 区域，这是因为，这样就表示了这页是持久被访问的，而不是某一时刻才会访问，这样就过滤了一些不会被持久访问的数据，也就是说，满足被访问和停留的时间超过一秒才会被放入 young 区域！

我们修改数据的时候，是直接在 buffer pool 中进行修改，然后设置为脏页，并不会直接刷入磁盘，同时我们会引入 redo log 来防止 MySQL 宕机的时候导致数据丢失，

## 15. redo log 是什么？

为了防止还没有落盘的 buffer pool 中的脏页数据在断电等故障的时候丢失，在我们更新了 buffer pool 之后，我们就会将做的修改操作记录到 redo log 文件里面。这里虽然也是写磁盘，但是这里并不是随机写，而是顺序写，**直接将数据追加到磁盘中**，所以速度很快，但是其实 redo log 也有一层 redo log buffer 定期将 redo log 顺序写入磁盘中，目的是减少磁盘 I/O ，尽管顺序写很快，但是我们依旧需要减少不必要的磁盘 I/O 来提升性能。

注意，我们记录的是对 buffer pool 的 redo log，也就是说，对于 undo log 的修改，我们也需要记录到 redo log 里面实现持久化！当然，并不是所有位于 buffer pool 的页都会记录，典型的像数据页，索引页，undo 页都会写 redo log 来保证持久性。

我们的 redo log 采取的是循环写模式，默认由两个 redo log file 组成，我们会先写满 file1，写满后再写 file2 ，然后又写 file1 这样的顺序。相当于一个环形。我们会用 `write pos` 表示当前写的位置，`checkpoint` 表示当前需要擦除的位置，如果 `write pos == checkpoint` 则无法写入 log ，会陷入阻塞。

`innodb_flush_log_at_trx_commit` 字段也需要注意，当我们事务提交之后，但是此时 redo log 会不会还没有刷盘呢？此时如果宕机，就会丢失已经 commit 但是还没有刷盘的数据：

- 我们将这个字段设置为 0 时，性能最高，每一秒会对 redo log 执行一次刷盘，也是最不安全的，可能会丢失数据；

- 字段为 1 时，性能最差，但是最安全，每次事务提交都会刷盘一次，系统默认该方式；

- 字段为 2 时，相对 0 更安全，每次都会将 log buffer 写入 logfile（这是操作系统的 Page Cache，这里意味着将数据写入了操作系统的内核空间，只有操作系统挂掉才会丢数据），但是刷盘依旧是每秒一次。

总的来说，为了保证数据安全性，我们使用 redo log 对所有 buffer pool 中需要崩溃恢复一致性的数据页进行统一记录日志，保证了数据的安全，但是同时为了提高事务执行效率，我们又使用了操作系统的 Page Cache，默认对于每个事务提交都会直接刷盘，但是如果对于性能有更高的要求，而对于数据安全性要求不高，可以通过修改配置来解决。

## 16. binlog 是什么？

上面的 redo log 用于故障恢复，而 binlog 则是用于备份和主从复制的。

同时 redo log 是循环写，数据库的日志并不会全量保存，而 binlog 则是将日志全量保存，即便我们将数据库的数据都删除了，也可以通过 binlog 恢复。

binlog 有三种文件格式 `STATEMENT`，`ROW`，`MIXED` ，`statement` 是默认文件格式，会记录 SQL 语句，而 `ROW` 会记录行数据的修改记录，我们的 `ROW` 在频繁的更新中会记录多个修改记录，而 `statement` 只会记录一个，所以第一种的占用更小，而第三种则是混合模式，根据不同的情况来做选择。

**那么是如何实现主从复制的？** 首先是主库写 binlog 日志，提交事务，更新本地存储数据，然后我们主库的 binlog 会复制到从库，将 binlog 写入到暂存日志，最后回放 binlog ，更新存储数据。对于主库来说，我们在主从复制的时候会创建 log dump 线程来处理复制的请求，同时也有网络带宽的限制，所以从库并不是越多越好。

这里的主从复制其实有三种模式，和 ack 机制很像：

> 1. 同步复制：从节点全部复制完才会返回客户端结果，缺点是可用性差。
> 2. 异步复制：无需等待，如果一旦出故障，数据就会丢失。
> 3. 半同步模式：同步到任意一个从库上就返回结果，兼顾了可用性和可靠性。

而我们的 binlog 也不是直接写入文件，而存在着一个内存缓冲区 binlog cache ，事务提交的时候，就会把对应的 binlog cache 写入到 binlog 文件，并清空 cache。

我们的 binlog 写入磁盘的流程如下（借用小林coding的图）：

![binlog cach](./assets/binlogcache.drawio.png)

我们的线程会将自己的 binlog cache 通过 write 系统调用写入到内核，此时是缓存在内核的 page cache 中，然后才会通过 fsync 持久化到磁盘，这里就是真正的磁盘 I/O 。

通过修改 `sync_binlog` 参数，我们可以决定磁盘 I/O 的时机：

> 1. 参数为 0 ：提交事务只 write ，而不调用 fsync，后续由操作系统决定何时将数据持久化。性能最好，但是风险最大。
> 2. 参数为 1 ：每次提交事务都会 write ，然后立刻调用 fsync。性能最差，最安全。
> 3. 参数为 n ：每次提交都 write ，事务累计 n 次就会 fsync。自定义。

和 redolog 对比一下，binlog 对于每个提交事务都至少会写入 page cache 中，对于写磁盘的限制也不相同，redolog 是定期触发，而 binlog 是积累一定的量会进行写磁盘，或者定期触发。

## 17. 两阶段提交是什么？

在我们仅仅使用 redo log 和 binlog 的时候，有可能会导致主从数据不一致，比如 binlog 已经持久化，但是 redo log 还没有持久化的时候宕机了，主节点的数据就延后了这种情况；另一种情况是redo log 已经写入磁盘之后，binlog 没有及时写入磁盘然后宕机了，导致从节点数据落后。

而解决这个问题的方法就是二阶段提交，通过使用 XA 事务来维护 redo log 和 binlog ，事实上，就是利用了 XA 事务的 ID 来进行判断，保证数据一致性：

**Prepare 阶段**：我们会将 XID 写入到 redo log 中，然后持久化到磁盘（需要设置 redo log 为提交事务就写入磁盘）

**Commit 阶段**：将 XID 写入到 binlog 中，然后持久化到磁盘，接着调用提交事务接口，将 redo log 设置为 commit ，但是只要当我们的 XID 写入 binlog 然后被持久化其实就已经算成功了，之后尽管是故障重启，也会认为事务执行成功。

当故障重启的时候，我们会读取 redo log 来恢复数据，此时如果读取到 prepare 状态的数据，我们会拿着这个数据对应的 XID 回到 binlog 去查看是否有相同的 XID ，如果有，就会恢复这部分数据，如果没有，就不会恢复，也就是说，在 binlog 持久化之前，如果故障了，此时的事务就会回滚，意味着事务失败，只要 binlog 成功了，那么就会恢复，这样就保证了一致性。这就是两阶段提交能够解决这个问题的原因。

**但是**，我们为了实现这个两阶段提交，必须要保证早点刷盘，将数据写入到磁盘里，如果频繁的进行磁盘 I/O ，性能肯定会非常差，同时，我们为了保证事务提交顺序，还需要加锁，这会导致激烈的锁竞争。

MySQL 为了应对这个问题，引入了**组提交**机制，当有多个事务提交的时候，将多个 binlog 刷盘合并成一个，从而减少磁盘 I/O 的次数，此时的 commit 会分为三个阶段，每个阶段有一个队列，有一个锁，这样可以减少了锁的粒度，提高性能并且保证了顺序性：

> - flush ：将 binlog 写入文件（不是刷盘）
> - sync ：刷盘操作
> - commit ：事务更新 commit 状态。

组提交和 binlog 的主从复制同步模式是互相叠加的，并不冲突，对于 redo log 其实也有组提交，它的组提交会延后到 flush 阶段进行刷盘，也就是 prepare 阶段和 flush 阶段合并了。

### 17.1 一些思考

对于这一系列八股，我们大概可以看出 mysql 的演进过程，最开始使用了 redo log 和 binlog 分离，存储引擎的 redolog 保证故障恢复，而 binlog 负责主从复制，然而这样分离的日志在崩溃的时候可能会导致不一致，然后 mysql 引入了二阶段提交，然后发现性能欠缺，又引入了组提交机制，虽然每一步演进都比较合理，但是不难看出 mysql 每一步演进都是为了兼容旧的系统而进行的更改，是在对旧架构上不断打补丁，渐渐地让整个系统架构比较复杂。

而 mysql 在之后针对于分布式集群又支持了 paxos 共识算法，弱化了 binlog 的功能，但是 redolog 依旧保留，保证存储引擎层的 WAL，即便理论上将 redolog 归到 paxos 中可行，但是我觉得应该是历史包袱过重的原因，才选择了保留。

而新一代的数据库比如 TiDB 底层直接使用的 raft 共识算法来实现集群复制，同时 raft 中的日志功能，也能够保证 WAL，将单机存储引擎的故障恢复和集群复制统筹起来，就没有了 mysql 层次多且复杂的设计结构，这样架构就简洁了许多。

## 18. 何时会出现 using filesort 和 using temporary?

大多数情况下，using filesort 更省事，如果可以在内存中完成，那么是比较快的，如果超出 `sort_buffer` 会落盘就会变慢，而 using temporary 为了 group by/union/distinct 等，也就是为了去重，分组，会将中间结果写入临时表再处理。

而两者并存时，会将中间结果写入临时表，在排序，开销更大。

**注：** filesort 并不直接表示文件排序，也不一定表示在磁盘中排序，只是在需要返回有顺序的结果时，没有对应的索引就会使用 filesort 进行排序，超出 `sort_buffer` 则会在磁盘临时文件中进行排序。

## 19. MySQL 里面如何基于 B+ 树进行查询的？

B+ 树只有叶子节点存放数据，而非叶子节点仅存放目录项作为索引，所有节点都按照索引键大小排序，构成双向链表，对于每一个节点，内部是通过单向链表组织的，为了加速查找，使用了 slot 当作页目录，存放每一个 slot 组的最大的索引元素，通过 slot 进行二分，找到对应的分组后进行遍历，找到对应的目录项后，到对应的叶子节点继续查找记录。

## 20. 为啥说单表不要超过 2 kw 数据？

这是一个经验之谈， 2 kw 行数据，而我们的每一页数据会有 16 kb，那么一页索引大约可以指向 1000 条子节点，算下来 2kw 大约是三层的 b+ 树，此时每次查询会有 3 次磁盘IO，性能会降低，这也就是经验之谈。

## 21. 分页查询会有什么性能问题？

分页查询有啥问题？首先，如果我们执行的是 `SELECT * FROM TABLE ORDER BY id LIMIT 60, 10` 时，会发生的事情时，我们会根据走的索引，去除 0 ~ (60 + 10) 条数据，然后根据我们 SQL 语句的 offset 进行去除，这就导致了我们获取了不必要的数据，导致了大量的消耗，当页数越来越多的时候，效率就会非常低下，我们可以通过嵌套查询，先根据分页查询仅查询 id 或者某一字段，然后再根据 id 范围查询即可，此时效率高在哪呢？数据拷贝会减少，仅此而已。

同时对于非主键索引分页查询，视情况会变成全表扫描，因为有的时候，回表次数会爆炸🤣，优化思路和上面一样，先拿主键再范围查询。

对于深度分页问题，我们的优化思路是将所有的数据按照主键排序，然后分批次，每次 where 范围查询 > 当前批次的第一个主键 id 进行查询，这样查询的效率就比较稳定，那我们如何拿到每页开头的 id 呢？我们每次查询分页的时候，将当前页的最大 id 作为下一页查询的条件即可。

当然，这样也仅能在瀑布流的页中使用，也就是不支持跳页的时候可以让查询速度很稳定，而如果需要支持跳页的话，依旧是个无法解决的问题。

## 22. 讲讲全局锁？

全局锁执行后，整个数据库会是只读状态，主要用于全库的逻辑备份，如果不加全局锁的情况下备份就会导致意外的情况发生。

但是此时又会导致数据库没办法写，也就是业务会不可用，所以我们可以利用 innodb 的可重复读的隔离级别进行备份，同时可以防止业务完全不可用的状态。而对于其他存储引擎，就没有办法了😂。

## 23. 意向锁有啥用？

意向锁主要是为了表锁和行锁的并发协调而设计的，加行锁或者表锁的时候都会为表加一个意向锁，意向锁和意向锁直接不会冲突，只会与表锁相冲突，比如我们需要将表中某一行记录锁上，此时会加上意向锁，然后有一个事件需要将整个表锁上，于是检查意向锁，发现被锁上了，于是等待，这里就不需要去一行一行遍历表中有没有被锁上的某一行数据，而可以直接判断。

也就是说，表锁和行锁并不直接互斥！而是通过意向锁来进行协调的。

## 24. 自增主键是咋实现的？

最开始，是通过一个表级锁 AUTO_INC 锁来为 AUTO_INCREMENT 修饰的字段实现自增的，如果同时插入，则其他 SQL 语句的执行会被阻塞从而保证字段连续且递增。

在之后的版本， innodb 提供了一个轻量级的锁来实现自增，这里只会为被 AUTO_INCREMENT 修饰的字段单独加上一个锁，来保证字段的连续自增。

通过 `innodb_autoinc_lock_mode` 可以控制使用哪一种锁：

> - 0 表示使用 AUTO_INC
>
> - 2 表示使用轻量的锁，但是申请到主键之后直接释放锁，不会等语句执行完毕。性能最高但是搭配 binlog 的日志格式是 statement 的时候会出现数据不一致的情况。比如，两个 session 开启事务，交错的 insert，此时各个事务生成的 id 并不连续，然而在 binlog 同步到从库时，确实线性执行你的事务，会导致申请的 id 不一样的问题，但是当使用 row 模式，记录的是行数据，此时就不需要担心数据不一致的问题了
>
> - 1 表示一般的 insert 语句依旧申请完就释放，但是对于批量的 insert 会等到语句结束再释放

## 25. 介绍一下行锁？

innodb 支持行锁，而 myisam 不支持。

行锁中，record lock 表示记录锁，把这条记录锁上，不允许其他线程修改。

gap lock 表示间隙锁，锁定一个范围，不锁定记录，不能插入数据，当然也不能删除，不同间隙锁之间并不互斥。

next-key lock 表示前两者结合，锁定范围包括记录，既不能插入，也不能修改数据，这个锁是互斥的。

插入意向锁，虽然名字里面有意向锁，但是它本质上是一种间隙锁，当需要对某个区间进行插入操作的时候，需要对这个间隙进行加锁，防止并发插入同一个间隙，以及防止插入被锁上的区域，产生幻读。

## 26. update 语句不加 where 会加表锁吗？

并不是表锁，而是由于需要对整个表进行扫描，于是会对每一行进行加锁，以及每个间隙加上间隙锁，所以会到达表锁的效果。

为了避免这种事故的发生，我们可以将 `sql_safe_updates` 参数设置为 1，表示 update 语句必须要满足一下条件之一才可以发生：

- 使用 where 且必须有索引列。
- 使用 limit。
- 同时使用 limit 和 where。

delete 语句必须满足以下条件：

- 同时使用 where 和 limit。

对于这种全表扫描的导致锁表的情况，也有可能是因为没有走索引导致的，优化器判断使用全表扫描，我们如果想要避免这种情况，也可以强制使用索引。

虽然 FORCE INDEX 可以避免锁全表，但是其也会造成性能的下降，并且效益也不会提高很多，所以应该谨慎一点。

## 27. MySQL 是怎么加锁的？

当我们针对某个主键索引的某一个区块进行加间隙锁或者加记录锁的时候，并不是只有这个主键索引会加锁，可以换个思路，其实我们插入数据的时候，这个插入的数据只要有一个栏目与加间隙锁的区间相关，就无法插入，这个与索引无关，而是与符合条件的插入数据或者更改数据有关，当然，这一点也可以理解为对其他索引也加锁了。

针对于**唯一索引等值查询**，如果对应的记录存在，会加上表级意向锁和行级记录锁，此时就算仅靠记录锁也能避免幻读；如果这个查询的记录不存在，那么会加上表级的意向锁和行级的间隙锁，此时，会阻塞对应区域的插入 SQL 语句，如果这里用临键锁，就会阻塞左右两边存在的数据的读写，这是错误的，所以应该使用临键锁，此时也能够避免幻读。

针对于**唯一索引范围查询**，会对每一个扫描到的记录加上临键锁，这是大多数情况下，其实范围查询加的锁是非常符合我们直觉的，无非就是关于边界的几种情况，对于闭区间，大于等于，且边界对应时，会加上记录锁，小于等于，且边界对应会使用临键锁，为什么？因为临键锁的模式就是 (l,r]，所以是这样加的锁，而当边界无法对应的时候，对于大于的情况会使用临键锁，而小于的情况会使用间隙锁，这里只需要理解我们的临键锁的加锁范围就可以很好理解这一点了。

针对于**非唯一索引等值查询**，如果对应的记录不存在，和唯一索引一样，会加上间隙锁，而对于等于边界的插入语句，有的情况会成功，因为插入的数据由于二级索引相同，所以可能在边界的左边，也可能是右边，所以就有了区别。同时，在做等值查询的时候，如果这个数值存在，和唯一索引的等值查询不一样，这里会加间隙锁和临键锁，因为这个索引值不一定唯一，我们需要阻止左右两边插入数据等值的情况，而如果只加记录锁是做不到这点的。

针对于**非唯一索引范围查询**，思路类似，不再赘述。

与此同时，对于非唯一索引查询，还会对扫描到的数据的主键索引加上记录锁，这一点很容易想明白。

当**没有加索引的查询**，会全表扫描，锁整张表。
