# 操作系统碎片知识

## 1. 预读失效和缓存污染是怎么解决的？

常见的 xv6， MySQL， Linux 在文件系统前面都有一层来缓存文件内容，实现对文件的快速访问，这里着重于 Linux ，和 MySQL 一样， linux 也没有采取简单的 LRU 替换策略，这里主要是因为传统的 LRU 可能会导致预读失效和缓存污染两个问题。

**预读机制**就是根据空间局部性原理，在加载磁盘中的一个页大小的数据的时候，会将相邻的数据页也加载进入 page cache ，但是如果一次性读取大量的数据页，但是他们其实之后并没有被读取，反而浪费了缓存空间，这就是预读失效问题，因此，我们需要尽量减少预读进来的页停留的时间尽量少，这一目的是和 MySQL 一样的。

我们的 Linux 将 LRU 分为了两个链表，活跃页和非活跃页链表，我们会将预读的页加入非活跃页链表，而活跃页则是至少读取过一次的页，非活跃页只有被读取了过一次，才会放入活跃页，否则，就会从非活跃页链表中溢出。

但是与此同时，我们如果一次性读取大量的数据，而这些数据仅仅会读取一次，就放入了活跃页链表，而之前会多次读取的热点数据就被加载的大量数据给淘汰了，但是这些数据仅仅会读取一次，很明显，这是不合理的，所以我们应该提高进入活跃页链表的门槛，在 Linux 中，只有在页被二次访问的时候，才会加入到活跃页链表，而 MySQL 中成为活跃页的要求更加苛刻，需要你访问的持续时间（第一次到最近一次的间隔）超过 1 秒，才能够加入 young 链表中，这也能够很好的减少缓存污染的问题。

## 2. 讲讲 Page Cache ?

这里不得不提 xv6 中 lock 实验的 `buffer cache`， 它用来充当文件系统的缓存，以磁盘块为单位，而我们的 `Page Cache` 是以页为单位，也是文件系统的缓存，在 linux 中， `buffer cache` 被合并在了 `Page Cache` 中。

我们的 `Page Cache` 在内核中是全局可见的，我们通过系统调用 `read`， `write` 等等都是在内核态中读写 `page cache` 来实现的，在这种情况下，由于直接读写内存，速度是非常快的，而且减少了磁盘 I/O，而当我们进程崩溃，并不会对文件系统或者 `page cache` 造成影响，但是如果操作系统崩溃，那就可能会导致数据丢失。

对于操作系统崩溃也有一些要说的，但是总结成一句话就是 log 日志可以保证操作系统的文件系统状态正常，但是无法保证数据不丢失。

## 3. 凭什么系统调用，进程级切换开销就大？

- 缓存（如TLB）失效，因为不同进程切换会导致 TLB 失效，TLB 用于加速虚拟地址到物理地址的转换，而 TLB 失效会导致访问延迟；

  除此之外由于不同进程的内存空间是隔离的，访问内存的区域不同，不符合缓存的空间局部性和时间局部性原理，导致缓存失效。
  
  另外说一点就是在 xv6 里面，由于进程之间在 tlb 中并没有什么特殊手段去区分，所以每次切换进程都会刷新 tlb，这样就导致每次切换的开销都是巨大的，然而在现代的操作系统里面，tlb 是可以通过 psid/asid 使得不用在切换进程的时候刷新，因为他为每个进程发放了一个标识符，就不需要再刷新了。
  
  在多核情况下，情况会更复杂，也就会有所谓的 shootdown 问题，详见[这篇文章][http://www.wowotech.net/process_management/context-switch-tlb.html]

- 保存和恢复上下文的开销。  
- 内核资源共享，引起比如文件句柄，锁等的资源竞争。

## 4. cpu 的缓存一致性问题

由于我们的读写操作会导致缓存和内存数据不一致，而解决方法采用的是**写直达**和**写回**两个策略。

目前我们目前的操作系统基本都是多核心的，对应的就会引起多核心的缓存一致性问题。为了解决这个问题，需要做到：

> 1. 某个 cpu 核心的缓存数据更新时，需要传播到其他核心的缓存中，也就是**写传播**。
> 2. 某个核心中对数据的操作顺序必须在其他核心看起来的顺序是一样的，也就是**事务的串行化**。

有一个实现就是**总线嗅探**，比方说 A 号核心修改了其缓存中的 x 变量，通过总线将这个时间广播给其他核心，而每个核心都会监听总线上的广播事件，并检查自己是否有相同的变量在自己的缓存中，但是并不能保证事务串行化，并且会很大程度上加重总线的负载。

而有一个 **MESI 协议**基于总线嗅探实现了事务串行化，由已修改、独占、共享、已失效四个状态转移来实现。

## 5. 内核线程和用户线程和协程？

用户线程其实就是在用户态实现的线程，由用户态的库函数进行调度，优点很明显，无需系统调用通过操作系统来操作线程，缺点也明显，当我们用户线程陷入系统调用的时候，我们的用户态的库函数没办法对其进行调度，整个进程都会被阻塞；

而内核线程则完全与上面相反，线程切换开销大，但是当陷入线程系统调用的时候，也可以进行调度，而不会被单一的线程阻塞。

而集两者优点于一身，诞生的就是**轻量级进程**！常见的实现就是我们 go 语言里面的协程了，它是由内核线程支持的用户级线程，内核线程和用户级线程如何对应？这里需要扯到我们常说的 GMP 模型，Go 语言里面的协程，它的用户级线程和内核线程是多对多的关系，而其他的实现还有一对一和多对一的关系。

## 6. 如何评价 malloc？
c 语言里面可以通过 malloc 动态分配内存，根据分配内存的大小，malloc 会通过 brk 和 mmap 两种分配内存，在分配的内存比较小的时候，比如小于 128 kb，会调用 brk 来进行分配，此时会从堆中去分配空间，然而，并不是说请求多少就分配了多少堆内存，它实际上还会预分配多一点内存到用户空间，这样在后续再调用 brk 的时候，就不会再进入到内核了，而是直接在用户态分配，直到分配的内存用完，才又回到内核继续分配。

针对于 brk，我们分配了内存之后，可能会调用 free 去释放某个区域的内存，但是实际上并不会归还给操作系统，因为有可能我们只归还了 1 个字节，这样不如直接缓存到 malloc 的内存池里面，再次申请 1 字节内存的时候就可以进行复用，又比如说。以上是八股文的说法，其实我觉得还有另一个原因，那就是我们申请页表是一整张，我们可能只会归还其中一部分，这样是无法将整个页表都归还的，所以干脆直接不归还，继续缓存到内存池里面。再者，如果频繁的归还小块内存，系统调用的开销也很大。同时，brk 的语义也应该不允许堆内存中间会出现空洞。

针对于 mmap，他每次分配内存都需要经过系统调用，开销比 brk 大得多，通常小对象会分配的比较频繁，所以针对于很大的对象才会通过 mmap 分配。如果大对象在 brk 分配的话，回收之后就会导致有很大的内存碎片了。

值得一提的是，通过 brk 申请了这块空间，也只有在真正用到的时候才会通过缺页错误进行真正的分配，从而减少了空间的浪费。

## 7. 调度算法

1. 先来先服务，效率低，不公平。

2. 最短作业优先，同上，长时间作业无法得到 cpu。

3. 高响应比优先，不切实际，无法预测进程运行时间。

4. 时间片调度，最公平，使用最广，如果时间片过短会导致上下文切换损耗过大。

5. 最高优先级调度，不公平，可能会导致低优先级被饿死。

6. 多级反馈队列：就是将时间片和最高优先级调度结合起来，相对来讲比较理想，所有就绪进程通过队列进行等待，cpu 从不同队列中获取任务执行时间片，如果进程等待的时间过长就放入时间片分配时间更长的队列。

## 8. 原子操作如何实现的？

1. 总线锁定，早期 cpu 比较暴力的做法，即处理器在总线上发送一个 LOCK# 信号，其他处理器的请求将被阻塞，此时这个处理器可以独占内存，此时其他处理器也不能操作其他内存地址的数据，所以开销很大。

2. 缓存锁定，当访问的内存缓存在处理器的缓存行中，并且在 LOCK 期间被锁定，那么此时不会通过总线输出 LOCK# 信号，