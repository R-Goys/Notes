# 计算机网络

## 1.我们的底层链路最终是通过 MAC 地址找到目标主机的，那么 IP 如何转换成 MAC 地址？

ARP 协议，当一台计算机知道目标主机的 IP 地址，而不知道其 MAC 地址时，会通过 ARP 请求广播到网络中：

1. 此时会像当前局域网广播一个 ARP 包，询问谁拥有这个 IP 地址？
2. 目标主机收到请求后，会发送一个 ARP 响应包，里面包含他的 MAC 地址。
3. 发送者收到 ARP 响应后，会将目标主机的 IP 地址和 MAC 地址存入 ARP 缓存。

ARP 协议并不基于任何协议，它属于数据链路层协议，当需要跨网络（不在同一局域网内），路由器会先根据目标 IP 地址进行路由选择。

## 2. 键入网址到页面显示发生了什么？

1. 第一步是解析 URL，**生成**我们需要发送给 web 服务器的请求信息，相当于输入（input），比如协议，web 服务器（地址），路径，文件等。

2. 此时我们得到了 web 服务器地址，比如说 www.baidu.com 这样的地址，我们需要将他解析成 IP 地址才可以去进行访问，我们本机虽然会有缓存，但是肯定无法存储所有域名对应的 IP 地址，此时就需要去向专门的 DNS 服务器去查询指定域名的服务器地址。

   此时额外需要注意的就是我们的域名以 `.` 分割，这就代表了层级，先向根 DNS 服务器查询 `.com` 的地址，再去 `.com` 服务器查询 `.baidu` 的地址，这样最终就找到了我们想要访问的服务器的地址。

3. http 调用套接字，通过 tcp 实现可靠传输。

4. 将源地址，目标地址，协议号（如 TCP）填入 IP 包头，生成 IP 报文。

5. 然后我们需要加上 MAC 头部，需要将目标 MAC 地址，源 MAC 地址，协议号填入，目标地址可以通过本地的 ARP 缓存表查询，如果本地没有，可以通过 ARP 协议获取目标地址的 MAC 地址。

6. 最后，包头加上起始帧分界符，包尾加上 `FCS` 帧校验序列之后，我们需要通过网卡就将一系列二进制数据转换为电信号发送出去。

7. 中途，交换机中，电信号会被转换为数字信号，校验错误，然后放入缓冲区，通过包头的目标端口转发到对应的交换机端口从而发送到链路中。

8. 经过路由器，它具有三层，具有 MAC 地址和 IP 地址，而交换机没有，所以他会对包头的 MAC 地址进行检验，不是发给自己的就丢弃，然后去掉 MAC 头部，根据 IP 头部信息进行转发。

   我们在途中每经过一个路由或者主机，都需要进行一次路由选择算法选择对应的 IP，然后通过 ARP 缓存表或者 ARP 协议获取对应 IP 的 MAC 地址，以此来转发包。

9. 到达服务器，去除一系列包头，到达应用层。

## 3. LINUX 的收发网络包

**DMA 是什么？** 我们需要将网卡中的数据写入内存时，会通过 DMA 技术将网卡中的数据写入指定的内存空间，此时不需要 cpu 的参与，这就是 DMA 技术，在 kafka 零拷贝，文件读写等经典八股场景我们也会看见它的身影。在写入指定的内存地址之后，我们的网卡会向 cpu 发起中断，随后我们会进入到内核保存之前执行的上下文之后执行指定的中断处理函数。

由于单纯依赖中断来检测网络数据包是否到达的效率过低，Linux 引入了 NAPI 机制，当数据包过多时，会先暂时屏蔽中断，而采用轮询的机制，定期检查是否有新数据包的到达，这样来防止频繁中断，从而提高效率。当然，我们发送数据的时候，也是通过 DMA 技术来让内核缓冲区中的数据拷贝到网卡，就不需要 cpu 的参与了。

## 4. Ping 和 traceroute？

ping 和 traceroute 都是基于 ICMP 协议工作的，而 ICMP 基于 IP 协议实现，由 IP 报头封装，属于网络层协议，可以向发送方返回错误信息，可以确认 IP 包是否送达目标地址，IP 包被丢弃的原因等，比如在数据包传输阶段通过 ARP 协议获取 MAC 地址失败，就会返回一个 ICMP 目标不可达数据包，其他类型的还有 TTL 降为零超时，端口未监听，找不到 IP，报文未分片等等。

traceroute 主要是发送 UDP 包，然后再通过给 IP 报头设置特殊的 ttl 来强制接受 ICMP 超时信息来拿到所有路由器的 IP，当然，有些路由器会保护自己的 IP，此时就会显示替换为 `*` ，并且 traceroute 会填入一个不可能的端口号作为目标端口，一旦接收到端口不可达的信息，就说明我们的发出的包到达了目标主机。

traceroute 的另一个作用是当前不知道数据链路的 MTU 是多少，从而去确定路径的 MTU，以便能到达目标主机。

而 ping 主要是通过发送 ICMP 查询报文从而实现的对端服务的 ICMP echo reply 返回请求。值得注意的是，ICMP 差错报文默认就是开启的，在发送网络包时出现问题时，会在需要时返回 ICMP 差错报文，而 ICMP 查询报文是通过客户端发送从而才可以进行查询的。

## 5. 0.0.0.0，localhost，127.0.0.1 到底有什么区别？

localhost 实际上是一个域名，但是默认会将其解析为 127.0.0.1，而 127.0.0.1 叫做回环地址，如果 ping 它的话，就会在本机网络协议栈转一圈回来，而我们 ping 本机地址也是一样的，而 0.0.0.0 在监听的时候代表的是本机 IPV4 的所有地址，而在 ping 的时候，表示的是无效地址，但是在本机 ping 的时候则是转换成了回环地址。

## 6. HTTP1.1 -> HTTP2 -> HTTP3？

HTTP2 相对于 HTTP1.1 做了大量优化：

HTTP2 首先采用了二进制帧格式，将原本整体传输的 HTTP 请求划分为多个具有固定结构的帧。多个 Stream 的帧可以在同一个 TCP 连接中交错传输，而同一个 Stream 内的帧仍保持顺序（一个请求只能在一个 stream 中传输），在对端再进行重组。这种分帧机制允许多个请求并发地共享一个连接，从而实现真正的多路复用，解决了 HTTP1.1 中由于串行请求导致的应用层队头阻塞问题。

此外，HTTP2 使用 HPACK 头部压缩算法，通过静态表和动态表对重复的 Header 字段进行索引化和压缩，大幅减少了头部体积，降低了网络传输开销。

尽管如此，HTTP2 依旧有着接收端的 tcp 队头阻塞问题，tls + tcp 三次握手的大量开销以及同一客户端在切换网络时需要重新建立连接的开销，在 HTTP3 中，这些问题得到了解决。

HTTP3 将传输层换成了 udp，同时在 UDP 的基础上的 QUIC 协议进行开发的，QUIC 协议将 tls1.3 集成到内部握手流程，同时无传输层队头阻塞问题，不依靠四元组重连的特性（同一客户端的连接快速迁移），同时也实现了 HTTP2 的流式传输。因此解决了以上的所有问题。

除此之外，HTTP/3 使用 QPACK 作为头部压缩算法。QPACK 基于 HPACK 的理念（静态表 + 动态表），其静态表的字段更多，意味着头部能得到更加充分的压缩；针对于动态表，由于请求的流是乱序的，所以难以依靠之前发送的请求的 header 生成的动态表来进行解析（因为 HTTP2 至少是有序到达对端的，所以不会有这种问题），所以 QUIC 设计了一个双向流用于同步双方的动态表的映射 KV，从而确保头部压缩在乱序环境下也能无阻塞地工作。

## 7. TCP 的流量控制和拥塞控制？

TCP 协议中，发送方在发送消息是需要考虑到接收方的接受能力来控制实际发送的数据量，这就是流量控制。

**为什么需要流量控制？** 如果发送方不考虑接收方的接受能力，不停地发送大量的数据包，就会导致对端无法在高负荷的情况下接受所有数据，因此就会丢弃从而触发重传机制，进而导致大量的带宽浪费。

TCP 的消息头有一个字段就是用于通知窗口大小的，接收端会将自己可用的接收数据缓冲区写入伴随 ack 返回给发送端。**那如果窗口满了咋办？** 此时发送方会时不时发送一个窗口探测报文来获悉最新的窗口大小。

然而，流量控制并不是说就是十全十美的，当接收端的处理能力跟不上时，窗口大小会不断减少，最终，当窗口大小缩小至 100 字节，甚至更小，引发的问题就是 TCP 每次发送数据只能发送很小的数据报，从而导致头部开销增大，因此引入了 Nagle 算法，在对端缓冲区小于一定数值的时候，接收端在返回 ack 的时候会通知对端当前窗口大小为 0 从而发送端就无法发送数据，直到接收端缓冲区大于一定阈值，并且发送端的数据包大于 MSS 才可以发送数据，当然不会一直阻塞下去，会有一个超时时间，如果延迟时间过长就会直接发送，这就避免了大量的小数据报的开销。提高了网络利用率；当然，如果对方 ack 很快，此时也满足 nagle 算法，就不会拼接小数据报直接发送。

**拥塞控制又是什么？** TCP 协议中，既然有一个窗口控制，意味着我们可以不等待应答就可以直接发送下一段数据，这也就意味着我们需要控制 TCP 报文的发送速度，我们如果设置为一个定值，设置得太小会导致网络资源利用不充分，太大又会导致不同时段的网络拥堵，容易超时，因此有了**拥塞控制**：
	1. 首先，TCP 在通信最开始会通过慢启动来对发送的数据量进行控制，这里定义了一个拥塞窗口的概念，最开始会将这个值设置为 1 MSS，意味着在没有收到 ack 的时候只能发送一个报文段，之后每次收到一次 ack，这个窗口的大小就 +1，意味着拥塞窗口会呈指数级不断变大。当然，这里最终的窗口值是取流量控制的窗口值和拥塞窗口的最小值，当然并不会一直都指数级增长，当超过拥塞窗口超过慢启动门限，就会使用拥塞避免算法，此时会接近线性增长的方式来增大拥塞窗口。
	2. 一旦产生超时重发，也就是说，当前 TCP 的拥塞窗口过大导致网络拥堵了，那么此时 TCP 的拥塞窗口又会变成 1 从而避免网络拥塞，但是由于之前的慢启动机制，我们的拥塞窗口又会很快变成原样，因此 TCP 设置了一个慢启动阈值，当触发超时重传时，TCP 会将慢启动阈值设置为当前拥塞窗口的一半。 ^af21c5

然而，在某些超时的情况下，我们可以使用快速恢复来减少拥塞窗口突然减小的影响。比如触发[[#^675ffb|快速重传]]时，TCP 认为这种超时并不严重，因此仅仅是将拥塞窗口变成原来的一半，而不是直接降低为 1。

其他提高网络利用率的方法：延迟应答（发送端发送多次报文的多次 ack 合并为一次 ack，在四次挥手中，如果开启了这个参数，并且缓冲区没有数据时，第二次和第三次回收会合并，从而变成三次挥手），捎带应答（ack 中携带一些返回信息）

当然，延迟应答和 Nagle 一起使用就会有问题，因为 nagle 需要等待 ack，而延迟应答如果收到小报文就会延迟等待，从而延迟 ack 的发送。

## 8. TCP 的半连接队列和全连接队列？

全连接队列的长度由 listen 的 backlog 参数和内核的 `/proc/sys/net/core/somaxconn` 参数决定，取最小值，如果在对方发最后一次 ack 的时候，全连接队列满了，有两种情况，一种是直接丢弃，一种是向对端发送 RST（connection reset by peer）来告知客户端连接建立失败，我们可以通过开启 `tcp_abort_on_overflow` 来做到这一点；默认条件可以提高建立连接的成功率，除非 TCP 队列长期是满的，则可以直接发送 RST 。^e1844c

半连接队列的长度不仅取决于内核的 `tcp_max_syn_backlog`，还和全连接队列的长度有关，逻辑如下： ^408f44
1. 首先判断半连接队列是否满了，半连接的长度 `max_qlen_log` 是上述全连接长度的最小值 * 2，如果满了且没有开启 `tcp_syncookies` 则直接丢弃。
2. 然后看全连接队列是否满了，如果全连接队列满了并且此时发送第一次握手但是没有返回 ack 的连接数大于 1，那么则会丢弃这个连接。
3. 最后看是否开启了 `tcp_syncookies`，如果开启了则直接通过了，如果没有，那么当 `tcp_max_syn_backlog` 减去当前半连接队列的长度小于 `tcp_max_syn_backlog >> 2` 则会丢弃。

由上可知，我们的半连接队列长度取决于 `tcp_max_syn_backlog` 以及全连接的相关队列参数。

顺便一提，TCP 的半连接队列实际上是一个哈希表，而全连接队列才是一个真正的链表，原因是我们第三次握手需要快速找到之前建立的半连接对象，而全连接队列只需要通过 accept() 随便取一个就可以了。

## 9. TCP 的优化？
1. 优化三次握手：
	- 客户端在第一次握手之后，如果没有收到 ack 会反复发送第一次握手，在某些情况下，我们可以通过修改 `tcp_syn_retries` 来减少重试次数，将错误快速反馈给应用层。
	- 服务端在第二次握手之后，如果没有收到 ack，这个还没有完全建立的连接会滞留在半连接队列里面，常见的有 syn 攻击就是攻击的这个半连接队列，我们可以通过上述的[[#^408f44|半连接队列介绍]]来调整半连接队列的大小，同时开启 `tcp_syncookies` 来实现不使用半连接队列的情况下建立连接。
	- 服务端在第三次握手的时候，全连接队列已满，默认会直接丢弃。我们可以通过[[#^e1844c|全连接队列介绍]]提到的方法调整策略以及全连接队列的长度。
	- 可以通过 TCP Fast Open 绕过 TCP 三次握手，快速建立连接，如图所示，当然，这种方法需要客户端和服务端都开启了 FastOpen 才行：
	  ![image](image1.png) ^c353bf
2. 优化四次挥手
	- 和三次握手很像，我们第一次挥手之后，如果迟迟收不到 ack，就会触发重传，我们可以调整 `tcp_orphan_retries` 参数来控制超时重传的次数，如果超出则直接关闭连接；当然除了这个之外，你需要 `shutdown` 和 `close` 的区别，当主动方调用 `close` （完全断开连接，关闭了读端和写端），他就叫做孤儿连接，这个时候我们的 `tcp_orphan_retries` 也对这个孤儿连接生效，他会不断处于 FIN_WAIT1 的状态，而且需要发送 FIN 报文，然而此时可能会因为之前内核中的数据还没有发送导致阻塞发送不出去，导致一直处于 `FIN_WAIT1` 状态，为了应对这种情况，我们需要适当调整 `tcp_max_orphans` 参数，大于这个参数之后新增的孤儿连接不会走四次挥手，而是直接发 RST 结束连接，以此来防止系统资源被持续占用。
	- 对于 close 关闭的孤儿连接， `FIN_WAIT2` 的状态不可以持续太久，所以有了 `tcp_fin_timeout` 来控制这个状态下连接的持续时间，如果超过就会直接关闭
	- `TIME_WAIT` 的存在（2MSL，60s）可以防止之后新建立的连接四元组接收到了旧连接还存活的包（时间够长）。在高并发情况下，可以调节 `tcp_max_tw_buckets` 参数防止 `TIME_WAIT` 状态过多，之后关闭的连接不再经历 `TIME_WAIT` 而直接关闭。因为 TIME_WAIT 的存在，导致我们无法用原来的四元组建立新的连接，对于这种情况，客户端可以通过开启 `tcp_tw_reuse` 来复用处于 `TIME_WAIT` 状态的 TCP 连接，从而减少等待时间，在这种情况下，需要开启 TCP 时间戳的支持，通过时间戳防止序列号回绕导致错误接收到旧连接的报文。
	- 以上的都是针对于主动方的优化，对于被动方有一个状态叫做 `CLOSE_WAIT`，如果程序中大量出现这个状态，可能是因为没有调用 `close` 函数，之后进入 `LAST_ACK` 状态，可以调整 `tcp_orphan_retries` 控制重发次数。

3. 传输性能提升
	- 之前提过了[[#7. TCP 的流量控制和拥塞控制？|流量控制]]，其中接收方在 ack 中会返回一个接受窗口，然而现在通信中，这个窗口可能不足以描述我们的窗口大小，我们可以通过设置 `tcp_window_scaling` 来扩展可以通知的窗口大小。
	- 我们还可以通过调整发送缓冲区 `tcp_wmem` 和接受缓冲区 `tcp_rmem` 来提高吞吐。发送缓冲区默认可以自动调整，而发送接收缓冲区需要开启 `tcp_moderate_rcvbuf` 来实现动态调整，同时也可以调整 `tcp_mem` 来提高 TCP 可以使用的内存。

如此一来，就可以让 TCP 在资源充裕时最大限度提高吞吐量，资源紧张的时候也可以动态地进行调整。


## 10. 如何评价 TIME_WAIT ？

### 10.1 TIME_WAIT 期间收到了相同四元组的 SYN 会发生什么？

此时如果开启了时间戳机制，当 SYN 大于服务器期望受到的 SYN 序列号并且时间戳比服务端最后受到的时间戳更大，则会重用这个四元组连接，跳过 `TIME_WAIT` 阶段。否则就会返回一个第四次挥手的 ACK 报文，客户端发现不对劲，就会发送 RST 报文给服务端。

当服务端在 `TIME_WAIT` 接收到 RST 报文时，`net.ipv4.tcp_rfc1337` 为 0 时，会直接关闭连接，为 1 时会丢弃这个报文。

### 10.2 如果客户端开启了 tcp_tw_reuse 会有什么问题？

开启 `tcp_tw_reuse` 时，往往也会开启时间戳机制来防止序列号回绕的情况；但是，针对于 RST 报文，是没有任何时间戳检测的，所以我们无法避免接收到过期的 RST 报文。

还有一个问题，如图所示，延迟的 RST 可能会导致连接无法正常关闭：
![image][assets/image2.png]

## 11. 如何评价 QUIC（基于 UDP 实现可靠传输）

### 11.1 TCP 存在的问题
	 
首先是为什么需要基于 UDP 实现可靠传输，这是因为我们仅仅使用 TCP 是存在着许多问题，由于 TCP 是直接写在内核里面，而不是应用层，如果想要进行改进升级，普及就会很困难。

当然不仅仅是这个原因，如果我们 TCP 本就完美无缺，那就不需要升级，现实是 TCP 存在着这样的缺陷：
	1. 队头阻塞问题
	2. 网络迁移需要重新握手
	3. 建立连接有延迟，比如我们常用的 https，我们的 tls 握手和 tcp 握手加起来时间太长了，即便是打开了 [[#^c353bf|FastOpen]] 也没法完全合并，更别提 FastOpen 是之后新加的特性，还没有普及。

因此，有了 QUIC 协议，基于 UDP 实现的可靠传输。

### 11.2 QUIC 协议

参考链接：[基于 UDP 实现可靠传输](https://www.xiaolincoding.com/network/3_tcp/quic.html)

针对于队头阻塞， HTTP2 是因为每个 stream 流都是跑在一个 tcp 连接上的，受到滑动窗口限制，而 QUIC 则是为每一个 stream 维护一个滑动窗口，所以就算阻塞也只有一个 stream 会被阻塞，不会影响其他 stream。

QUIC 的流量控制继承了 TCP 的窗口，但是有很多不同，对于每个 stream 有一个流量控制窗口，而对于一个 Connection 也有一个缓冲区限制，这就是针对于所有 stream 了；而拥塞控制直接将 TCP 的拥塞控制搬过来了，当然也支持其他的算法。

更快的连接建立，QUIC 协议内部直接包含了 TLS ，所以 QUIC 只需要三次握手就可以建立 TLS 加密连接了，同时，我们报头识别连接是使用的连接 id 而不是传统的四元组，所以连接迁移和恢复连接都只有很小的开销。

## 12. TCP 和 UDP 没有监听端口时，发送报文的区别。

针对于服务端没有监听端口的情况，如果此时客户端向没有监听的端口发送数据，TCP 是通过返回 RST 错误，而 UDP 则是通过 ICMP 返回端口不可达的差错报文信息，两者不一样。

同时，traceroute 也正是通过 UDP 会返回 ICMP 的差错报文信息来进行实现的，除了追踪沿路的路由器之外，还有一个作用就是设置数据报文不分片，如果丢失了就减少报文的长度，这样从而确定一个合适的 MTU 值。

## 13. 如何评价 HTTPS？

单纯的非对称加密无法同时做到避免窃听，篡改，冒充这几个风险。因为私钥只能有一方持有，如果用私钥加密，公钥解密，无法避免被窃听的风险，而如果用私钥解密，公钥加密又不能避免冒充/篡改的风险，所以我们为了保证 http 无法被窃听，篡改，加密，最终是使用非对称加密做身份校验 + 协商一个只有双方知道的的密钥，然后用对称加密进行通信，从而实现防窃听，防篡改，防冒充的目的。

全过程：
1. 在 TCP 三次握手完成之后，我们的客户端会向服务端发起加密通信请求，内容包括 tls 版本，客户端的随机数，支持的加密算法，这里是没有加密的。
2. 服务端受到客户端的请求后，也会生成一个随机数，tls版本，加密算法以及**数字证书**返回给客户端，这样，就有了两端就有了对应的随机数。
3. 数字证书中包含了服务端的公钥，为了确认这个数字证书的真实性，我们可以向浏览器/操作系统的 CA 确认，如果真实，那么我们可以再生成最后一个随机数，通过公钥加密返回给服务端，服务端可以解密来获取这个随机数。**思考一下，这里有没有可能出现中间人劫持？** 是有可能的，但是这里如果中转站希望从客户端哪里得到 preMaster key，也就是第三个随机数，必须要将自己的数字证书发给他，而不能发服务端的数字证书，这样才能正常的解密 preMaster key，但是这样的话，客户端服务器并不会信任这个数字证书，因此会弹出警告，所以这并不能说 https 不可靠。
4. 然后，服务端反馈一个“转加密通信”的信息以及给上述的握手信息做个摘要，**这一步之后，双方各自根据三个随机数计算出一个密钥，然后直接通过对称加密进行通信。**


## 14. 讲讲 CUBIC 和 BBR？

前面提到的有[[#^af21c5|快速恢复的 TCP 拥塞控制算法]]叫做 reno，下面来介绍一下 CUBIC 和 BBR 这两种拥塞控制算法。

reno 有着快速恢复，相对每次遇到丢包超时都直接进行慢启动有一定程度的优化，但是相比于当然 linux 默认的 CUBIC 以及 google 推出的 BBR 还是相形见绌了。

CUBIC 的拥塞控制窗口函数是一个三次函数，并且独立于 RTT 进行增长，相比于传统的方法，每一次 RTT 才能加 1 更加灵活，如果碰上高带宽环境，可能需要经过很多个 RTT ，拥塞窗口才能接近一个 `BDP`（带宽延迟积），而 CUBIC 在快速恢复阶段会激进地增长窗口的大小（三次函数），迅速回到上一次稳定点的附近，而 `w_max` 表示上次拥塞窗口发生丢包的点，也就是这个三次函数的拐点，靠近这个 `w_max` 时，增长幅度会减小，如果超过了 `w_max` 也就是超过了拐点，增长速度又会逐渐增大。

CUBIC 在上次瓶颈点附近增长会相对缓慢，但是相距较远的时候会迅速的增长，同时他的增长只与时间有关，更加公平，在高带宽，高延迟的场景下，CUBIC 能够很好的进行工作。

而由 google 设计的 BBR 算法则是做了更加全面的升级，在以前的算法中，我们将丢包等同于网络拥塞，也就是说网络阻塞，在以前，路由器内存比较小，速度也比较慢，所以数据包一旦变多，路由器就会开始丢包，那个时候算是比较合理的，但是现在路由器内存变大了，速度更快，丢包的原因更多可能并不是因为网络堵塞，所以 BBR 抛弃了基于丢包的方法，另外开辟了一个拥塞控制算法。

`BtlBw` 表示网络瓶颈带宽，也就是没发生丢包的最大的窗口，`RTprop` 表示往返传播时间，表示没有排队状况下，数据包往返需要多久。而他们的乘积就是上述所说的 `BDP`。传统的方法是根据在 `BDP` 附近或者更大的传输速率进行传输的过程中进行试探依靠是否丢包来调整发送速度。 而 BBR 则通过一种探测方式，在最开始会快速启动，找到网络的瓶颈带宽以及 `RTprop`，在这个阶段等到发送的 RTT 开始变小，那么就会进入到第二个阶段，降低发送的速率，等在途数据量降低到正常水平，就会进入到探测阶段，他分有 8 个周期，先增大速率看看带宽有没有增长，然后减少速率，把上一周期多出来的队列排空，后面六个周期正常发送。第四个阶段，通过在途数据量降低，测量最新的 `RTprop`。

在 BBRv2 中，他会观察 `inflight` （在途数据量） 和 `deliveryRate`（交付速率）的关系，如果前者在增大，而后者没有增大，那么说明此时网络正变得拥塞，此时会降低自己发送的速率。

好复杂，后面有空再整理一下把，现在是直接问的AI整理的🤣

## 15. TCP 重传机制

**超时重传**，当数据包丢失或者确认应答丢失，发送方没有收到 ack，时间一长就会触发超时重传，RTO 太长或太短都会引发问题，所以有了**快速重传**算法，比如在发送窗口大于 1 MSS 的时候，会发送多条记录，如果其中一条报文丢失了，那么接受之后的报文都会 ACK 丢失的报文的序列号，从而让对方快速重传，当然，此时也不知道应该重传一个丢失的报文，还是丢失之后所有的报文，为了解决这个问题，就有了 **SACK**（选择性确认），linux 2.4 之后默认开启，他会在 TCP 头部添加一个 SACK 的东西，将已经接收到的数据的信息发送回去，以此来告知对方哪些数据没有收到。 ^675ffb

**Duplicate SACK**，通过 SACK 告诉对方哪些数据被重复接收了，什么场景下会用到？在发送方连续发送了多个数据报，此时这些数据报延迟了，然后最后一个数据报文到达，接收方返回最开始的序列号的 ACK，然后此时对方触发快速重传，恰好这个时候，之前的数据报文全部到了，然后接收方接收到这些重复的数据报文时会通过 D-SACK 告知对方数据重复了，然后此时会将需要的最新的数据的序列号返回回去。